{
  
    
        "post0": {
            "title": "My Python/Pandas note for data processing",
            "content": "This is my personal Python . Pandas . import numpy as np import pandas as pd . Indexing . Naming . Group . Timestamp . print(pd.Timestamp(&quot;2021-03-07&quot;,tz=&#39;America/Indianapolis&#39;).week) print(pd.Timestamp(&quot;2021-03-07&quot;,tz=&#39;America/Indianapolis&#39;).dayofweek) print(pd.Timestamp(&quot;2021-03-08&quot;,tz=&#39;America/Indianapolis&#39;).week) print(pd.Timestamp(&quot;2021-03-08&quot;,tz=&#39;America/Indianapolis&#39;).dayofweek) . 9 6 10 0 . pd.Timestamp(&quot;2021-03-07&quot;,tz=&#39;America/Indianapolis&#39;).dayofweek . 6 . import pytorch . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-20-d35c46f8d1a2&gt; in &lt;module&gt; -&gt; 1 import pytorch ModuleNotFoundError: No module named &#39;pytorch&#39; . Python . List comprehension with if . x = [1,2,3,4,5,4,3] [&quot;Good&quot; if i&gt;=4 else &quot;Neutral&quot; if i==3 else &quot;Bad&quot; for i in x] . [&#39;Bad&#39;, &#39;Bad&#39;, &#39;Neutral&#39;, &#39;Good&#39;, &#39;Good&#39;, &#39;Good&#39;, &#39;Neutral&#39;] . OOP simple example . class Animal: def __init__(self, name): self.name = name # Create a class Mammal, which inherits from Animal class Mammal(Animal): def __init__(self, name, animal_type): self.animal_type = animal_type self.name=name # Instantiate a mammal with name &#39;Daisy&#39; and animal_type &#39;dog&#39;: daisy daisy = Mammal(&quot;Daisy&quot;, &quot;dog&quot;) # Print both objects print(daisy) print(daisy.animal_type) print(daisy.name) print(dir(daisy)[-3:]) . &lt;__main__.Mammal object at 0x00000195B15200D0&gt; dog Daisy [&#39;__weakref__&#39;, &#39;animal_type&#39;, &#39;name&#39;] .",
            "url": "https://ecosang.github.io/blog/cheatsheet/pandas/python/2021/02/22/My-Python-Pandas-note.html",
            "relUrl": "/cheatsheet/pandas/python/2021/02/22/My-Python-Pandas-note.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "My Tidyverse note",
            "content": "suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(purrr)) suppressPackageStartupMessages(library(lubridate)) suppressPackageStartupMessages(library(data.table)) . dplyr . Various dplyr sample codes . rleid . mtcars%&gt;%as_tibble()%&gt;%mutate(rleid=(gear!=lag(gear,default = 0)))%&gt;%mutate(rleid=cumsum(rleid))%&gt;%head() . mpgcyldisphpdratwtqsecvsamgearcarbrleid . 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 | 1 | . 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | 1 | . 22.8 | 4 | 108 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | 1 | . 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | 2 | . 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 | 2 | . 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 | 2 | . rleid with index . mtcars%&gt;%as_tibble()%&gt;% mutate(rleid=(gear!=lag(gear,default = 0)))%&gt;% mutate(rleid=cumsum(rleid))%&gt;% group_by(rleid)%&gt;% mutate(row_number=row_number())%&gt;% ungroup()%&gt;%head() . mpgcyldisphpdratwtqsecvsamgearcarbrleidrow_number . 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 | 1 | 1 | . 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | 1 | 2 | . 22.8 | 4 | 108 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | 1 | 3 | . 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | 2 | 1 | . 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 | 2 | 2 | . 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 | 2 | 3 | . find repeated numbers . find_duplicated_seq&lt;-function(vec){ #find repeated numbers but exclude -1 lead_vec=dplyr::lead(vec,default=0) out=as.numeric(vec==lead_vec&amp;vec!=-1) out=as.numeric(out+dplyr::lag(out,default=0)!=0) return(out) } df=tibble(vec=c(1,2,4,5,5,5,6.1,6,6.2,-1,-1,-1,8,8,8)) df%&gt;%mutate(out=find_duplicated_seq(vec))%&gt;%head(15) . vecout . 1.0 | 0 | . 2.0 | 0 | . 4.0 | 0 | . 5.0 | 1 | . 5.0 | 1 | . 5.0 | 1 | . 6.1 | 0 | . 6.0 | 0 | . 6.2 | 0 | . -1.0 | 0 | . -1.0 | 0 | . -1.0 | 0 | . 8.0 | 1 | . 8.0 | 1 | . 8.0 | 1 | . impute n numbers of consecutive NAs . # df=tibble(c=vec) # df=df%&gt;%mutate(rn=row_number()) # if(is.na(df$c[length(df$c)])){ # df$c[length(df$c)]=tail(df$c[!is.na(df$c)],1) # } # if(is.na(df$c[1])){ # df$c[1]=df$c[!is.na(df$c)][1] # } # tdf=df%&gt;%filter(!is.na(c)) # tdf_diff=tdf$rn%&gt;%diff() # len_na=tdf_diff[tdf_diff!=1]-1 # miss_idx=tdf$rn[(tdf_diff!=1)] # start_idx=miss_idx+1 # end_idx=start_idx+len_na-1 # start_idx_ip=start_idx[len_na&lt;dt] # end_idx_ip=end_idx[len_na&lt;dt] # start_idx_na=start_idx[len_na&gt;=dt] # end_idx_na=end_idx[len_na&gt;=dt] # dfi=df%&gt;%mutate(c=imputeTS::na.interpolation(df$c)) # for(i in 1:length(start_idx_na)){ # dfi$c[start_idx_na[i]:end_idx_na[i]]=df$c[start_idx_na[i]:end_idx_na[i]] # } # return(dfi$c) # } # cc=c(1,2,NA,NA,5,6.1,6,NA,NA,NA,NA,NA,7,7.1,7.2,7.3,NA,NA,NA) # impute_na_dt(vec=cc,dt=4) . slicing . Slice dataframe by row number. Negative for disselection. n() for the last row https://dplyr.tidyverse.org/reference/slice.html . mtcars%&gt;%as_tibble()%&gt;%slice(1:5)%&gt;%slice(c(-1,-n())) . mpgcyldisphpdratwtqsecvsamgearcarb . 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | . 22.8 | 4 | 108 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | . 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | . transpose of dataframe . mapping=tibble(channel=c(&quot;c01&quot;,&quot;c02&quot;,&quot;c03&quot;,&quot;c04&quot;,&quot;c05&quot;), type01=c(&quot;ahu1&quot;,&quot;disposal&quot;,&quot;ahu2&quot;,&quot;kitchen1&quot;,&quot;microwave&quot;), type02=c(&quot;ahu1&quot;,&quot;disposal&quot;,&quot;kitchen1&quot;,&quot;ahu2&quot;,&quot;light&quot;), type03=c(&quot;ahu1&quot;,&quot;ahu2&quot;,&quot;disposal&quot;,&quot;kitchen1&quot;,&quot;room&quot;), type04=c(&quot;disposal&quot;,&quot;ahu1&quot;,&quot;ahu2&quot;,&quot;kitchen1&quot;,&quot;living&quot;)) head(mapping) print(&quot;Transposed dataframe&quot;) # should be updated for pivot_wider / pivot_longer when R 4.0 is available. mapping %&gt;% gather(key = var_name, value = value, -1) %&gt;% spread_(key = names(mapping)[1],value = &#39;value&#39;)%&gt;%head() . channeltype01type02type03type04 . c01 | ahu1 | ahu1 | ahu1 | disposal | . c02 | disposal | disposal | ahu2 | ahu1 | . c03 | ahu2 | kitchen1 | disposal | ahu2 | . c04 | kitchen1 | ahu2 | kitchen1 | kitchen1 | . c05 | microwave | light | room | living | . [1] &#34;Transposed dataframe&#34; . var_namec01c02c03c04c05 . type01 | ahu1 | disposal | ahu2 | kitchen1 | microwave | . type02 | ahu1 | disposal | kitchen1 | ahu2 | light | . type03 | ahu1 | ahu2 | disposal | kitchen1 | room | . type04 | disposal | ahu1 | ahu2 | kitchen1 | living | . select columns by conditions . df=tibble(x=c(1,0,0,1),y=c(1,2,3,4),z=rep(NA_real_,4),x1=c(2,3,2,3)) print(&quot;all df&quot;) df print(&quot;select columns that do not have any NA value and two unique values&quot;) df%&gt;%select_if(list(~(length(unique(.))==2)&amp;(!anyNA(.)))) df%&gt;%select_if(list(~(n_distinct(.)==2)&amp;(!anyNA(.)))) . [1] &#34;all df&#34; . xyzx1 . 1 | 1 | NA | 2 | . 0 | 2 | NA | 3 | . 0 | 3 | NA | 2 | . 1 | 4 | NA | 3 | . [1] &#34;select columns that do not have any NA value and two unique values&#34; . xx1 . 1 | 2 | . 0 | 3 | . 0 | 2 | . 1 | 3 | . xx1 . 1 | 2 | . 0 | 3 | . 0 | 2 | . 1 | 3 | . rolling joins through data.table . ecobee_changes &lt;- data.table( change_id = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;), change_time = ymd_hms(c(&quot;2020-11-10 15:05:00&quot;, &quot;2020-11-10 16:15:00&quot;, &quot;2020-11-10 16:20:00&quot;, &quot;2020-11-10 17:30:00&quot;)) ) interactions &lt;- data.table( interaction_id = c(&quot;i1&quot;,&quot;i2&quot;,&quot;i3&quot;), interaction_time = ymd_hms(c(&quot;2020-11-10 16:17:30&quot;, &quot;2020-11-10 16:19:25&quot;, &quot;2020-11-10 17:31:30&quot;)) ) . ecobee_changes[, action_time := change_time] interactions[, action_time := interaction_time] setkey(ecobee_changes, &quot;action_time&quot;) setkey(interactions, &quot;action_time&quot;) out=as_tibble(ecobee_changes)%&gt;%select(-action_time)%&gt;% left_join(as_tibble(ecobee_changes[interactions, roll = &quot;nearest&quot;]),on=c(&quot;change_time&quot;,&quot;change_id&quot;)) out #interactions[ecobee_changes, roll = &quot;nearest&quot;]%&gt;%as_tibble()%&gt;%filter(distinct()) . Joining, by = c(&#34;change_id&#34;, &#34;change_time&#34;) . change_idchange_timeaction_timeinteraction_idinteraction_time . c1 | 2020-11-10 15:05:00 | NA | NA | NA | . c2 | 2020-11-10 16:15:00 | 2020-11-10 16:17:30 | i1 | 2020-11-10 16:17:30 | . c3 | 2020-11-10 16:20:00 | 2020-11-10 16:19:25 | i2 | 2020-11-10 16:19:25 | . c4 | 2020-11-10 17:30:00 | 2020-11-10 17:31:30 | i3 | 2020-11-10 17:31:30 | . x = data.table( eventid=c(1,2,3), start =mdy_hms(c(&#39;10/1/2016 04:30:00&#39;,&#39;10/1/2016 18:02:00&#39;,&#39;10/2/2016 14:21:00&#39;)), end =mdy_hms(c(&#39;10/1/2016 05:43:00&#39;,&#39;10/2/2016 01:23:00&#39;,&#39;10/4/2016 08:54:00&#39;)) ) #y is a data table with a list of all dates y = data.table( date =c(&#39;10/1/2016&#39;,&#39;10/2/2016&#39;,&#39;10/3/2016&#39;,&#39;10/4/2016&#39;,&#39;10/5/2016&#39;,&#39;10/6/2016&#39;), start=mdy_hms(c(&#39;10/1/2016 00:00:00&#39;,&#39;10/2/2016 00:00:00&#39;,&#39;10/3/2016 00:00:00&#39;,&#39;10/4/2016 00:00:00&#39;,&#39;10/5/2016 00:00:00&#39;,&#39;10/6/2016 00:00:00&#39;)), end =mdy_hms(c(&#39;10/1/2016 23:59:59&#39;,&#39;10/2/2016 23:59:59&#39;,&#39;10/3/2016 23:59:59&#39;,&#39;10/4/2016 23:59:59&#39;,&#39;10/5/2016 23:59:59&#39;,&#39;10/6/2016 23:59:59&#39;)) ) #set the key on y to match with setkey(y,&quot;start&quot;,&quot;end&quot;) #use the foverlaps function to match # #note that eventid 1 matches one date only # eventid 2 matches two dates # eventid 3 matches three dates # result &lt;- foverlaps(x,y,type=&quot;any&quot;) #show results x y result . eventidstartend . 1 | 2016-10-01 04:30:00 | 2016-10-01 05:43:00 | . 2 | 2016-10-01 18:02:00 | 2016-10-02 01:23:00 | . 3 | 2016-10-02 14:21:00 | 2016-10-04 08:54:00 | . datestartend . 10/1/2016 | 2016-10-01 | 2016-10-01 23:59:59 | . 10/2/2016 | 2016-10-02 | 2016-10-02 23:59:59 | . 10/3/2016 | 2016-10-03 | 2016-10-03 23:59:59 | . 10/4/2016 | 2016-10-04 | 2016-10-04 23:59:59 | . 10/5/2016 | 2016-10-05 | 2016-10-05 23:59:59 | . 10/6/2016 | 2016-10-06 | 2016-10-06 23:59:59 | . datestartendeventidi.starti.end . 10/1/2016 | 2016-10-01 | 2016-10-01 23:59:59 | 1 | 2016-10-01 04:30:00 | 2016-10-01 05:43:00 | . 10/1/2016 | 2016-10-01 | 2016-10-01 23:59:59 | 2 | 2016-10-01 18:02:00 | 2016-10-02 01:23:00 | . 10/2/2016 | 2016-10-02 | 2016-10-02 23:59:59 | 2 | 2016-10-01 18:02:00 | 2016-10-02 01:23:00 | . 10/2/2016 | 2016-10-02 | 2016-10-02 23:59:59 | 3 | 2016-10-02 14:21:00 | 2016-10-04 08:54:00 | . 10/3/2016 | 2016-10-03 | 2016-10-03 23:59:59 | 3 | 2016-10-02 14:21:00 | 2016-10-04 08:54:00 | . 10/4/2016 | 2016-10-04 | 2016-10-04 23:59:59 | 3 | 2016-10-02 14:21:00 | 2016-10-04 08:54:00 | . purrr . Purrr is useful when to wrap a function for a tibble with vectorization. . map might be a general language, but I use pmap_x for general use. . mtcars%&gt;% as_tibble()%&gt;% mutate(new=pmap_dbl(list(mpg,cyl), function(x,y){x*y+y^2+2}))%&gt;%head() . mpgcyldisphpdratwtqsecvsamgearcarbnew . 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 | 164.0 | . 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | 164.0 | . 22.8 | 4 | 108 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | 109.2 | . 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | 166.4 | . 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 | 215.6 | . 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 | 146.6 | . custom_if_else=function(x,y){ if (x==6 &amp; y&lt;=21){ out=&quot;type1&quot; }else if (x==8 &amp; y&lt;18.5){ out=&quot;type2&quot; }else{ out=&quot;type3&quot; } return (out) } . mtcars%&gt;% as_tibble()%&gt;% mutate(type=pmap_chr(list(mpg,cyl), function(x,y){custom_if_else(x,y)}))%&gt;%head() . mpgcyldisphpdratwtqsecvsamgearcarbtype . 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 | type3 | . 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | type3 | . 22.8 | 4 | 108 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | type3 | . 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | type3 | . 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 | type3 | . 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 | type3 | . Lubridate . Find time difference . start_time=ymd(&quot;2021-01-01&quot;,tz=&quot;America/Indianapolis&quot;) end_time=ymd_hms(&quot;2021-01-03 23:55:00&quot;,tz=&quot;America/Indianapolis&quot;) . print(end_time-start_time) print(difftime(end_time,start_time)) print(as.numeric(as.duration(end_time-start_time),units=&#39;minutes&#39;)) print(interval(start_time,end_time)/minutes(1)) . Time difference of 2.996528 days Time difference of 2.996528 days [1] 4315 [1] 4315 . Create a time-grid . Create 5 minutes interval timestamp from start_time to end_time. . start_time=ymd(&quot;2021-01-01&quot;,tz=&quot;America/Indianapolis&quot;) end_time=ymd_hms(&quot;2021-01-03 23:55:00&quot;,tz=&quot;America/Indianapolis&quot;) minute_diff=interval(start_time,end_time)/minutes(1) . # it creates time_grid=start_time+(0:(minute_diff/5))*dminutes(5) print(time_grid[1:2]) print(tail(time_grid,2)) . [1] &#34;2021-01-01 00:00:00 EST&#34; &#34;2021-01-01 00:05:00 EST&#34; [1] &#34;2021-01-03 23:50:00 EST&#34; &#34;2021-01-03 23:55:00 EST&#34; . print(sessionInfo()) . R version 3.6.3 (2020-02-29) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 19042) Matrix products: default locale: [1] LC_COLLATE=English_United States.1252 [2] LC_CTYPE=English_United States.1252 [3] LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C [5] LC_TIME=English_United States.1252 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] data.table_1.12.2 lubridate_1.7.4 forcats_0.5.0 stringr_1.4.0 [5] dplyr_0.8.0.1 purrr_0.3.4 readr_1.4.0 tidyr_0.8.3 [9] tibble_2.1.1 ggplot2_3.1.1 tidyverse_1.2.1 loaded via a namespace (and not attached): [1] pbdZMQ_0.3-3 tidyselect_0.2.5 repr_0.19.2 haven_2.3.1 [5] lattice_0.20-38 colorspace_1.4-1 generics_0.0.2 vctrs_0.3.5 [9] htmltools_0.3.6 base64enc_0.1-3 rlang_0.4.9 pillar_1.4.7 [13] glue_1.4.2 withr_2.1.2 modelr_0.1.8 readxl_1.3.1 [17] uuid_0.1-2 lifecycle_0.2.0 plyr_1.8.4 munsell_0.5.0 [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.3 evaluate_0.13 [25] broom_0.5.2 IRdisplay_0.7.0 Rcpp_1.0.1 scales_1.0.0 [29] backports_1.1.4 IRkernel_0.8.15 jsonlite_1.6 hms_0.5.3 [33] digest_0.6.18 stringi_1.4.3 grid_3.6.3 cli_1.1.0 [37] tools_3.6.3 magrittr_1.5 lazyeval_0.2.2 crayon_1.3.4 [41] pkgconfig_2.0.2 ellipsis_0.3.1 xml2_1.2.0 assertthat_0.2.1 [45] httr_1.4.0 rstudioapi_0.13 R6_2.4.0 nlme_3.1-139 [49] compiler_3.6.3 .",
            "url": "https://ecosang.github.io/blog/r/tidyverse/2020/12/12/My-R-Tidyverse-note.html",
            "relUrl": "/r/tidyverse/2020/12/12/My-R-Tidyverse-note.html",
            "date": " • Dec 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Sang woo Ham. I am currently a Postdoctoral researcher at Lawrence Berkeley National Laboratory (LBNL), Berkeley, CA. . My goal is to be a responsible, polymathic, and a practical engineer. The completion of tasks under trust and pursuing sustainability for the Earth are my top responsibility/priority. Learning/applying new things give me what to do when I confront challenges. Based on the responsibility and experience, I would like to bring the research into real world practice by connecting multiple stakeholders such as researchers, engineers, developers, etc. . I am interested in Building energy analysis/modeling by using R/Python through various Probabilistic models with Pyro/Stan/Pymc3 and physics-informed black-box models.. . My current research is providing a scalable MPC software for small and medium commercial buildings. This work includes: (1) improved system identification, (2) hybrid modeling approach for unmeasured disturbacne prediction, and (3) integrated open-source software development. . Curriculum vitae . Download at this page .",
          "url": "https://ecosang.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Research",
          "content": "Timeline | Sociotechnical Systems to Enable Smart and Connected Energy-Aware Residential Communities Project Vision | Field Data Collection | Data-driven Building Energy Normalization | Online Model for Unit-level Heating and Cooling Energy Predicition | Scalable Building Energy Model using Probabilistic Deep learning | . | Liquid desiccant and dew point evaporative cooling based 100% outdoor air system | Indirect Evaporative Cooler Modeling and Test Chamber Design | Data Center Cooling | . Timeline . . . . Sociotechnical Systems to Enable Smart and Connected Energy-Aware Residential Communities . Project Vision . . 3-year smart home project with community partners (IHCDA, BWI, Purdue) | A major student for idea development, figure works, and proposal writing assistance. | Project management role such as sensor procurement, meeting arrangement, communication between multiple partners/students/faculties. | . . Field Data Collection . . Prototyping and choosing WiFi-based thermostats and circuit-level power sensors and their data collection to a database. | Installation and trouble shooting of sensors for 50 residential units located in Indianapolis, IN. | Documentation of data communication and database schema to work with software developers to accommodate multiple sensors communication in real-time. | Development of R (Shiny) and HTML based interactive data analytics tool for time-series data (for internal-use). | Procurement/installation/design of Tablet/Smart speaker for smart home interface (SmartE) with research team. | . . Data-driven Building Energy Normalization . . “A data-driven building energy normalization model for eco-feedback design in smart and connected multi-family residential buildings.” | Energy comparison of residential units in a multi-family residential building as an eco-feedback. | Normalized groups by considering inter-unit heat transfer and unobserved disturbances. | Fair comparison of the impact of thermostat behavior on heating/cooling energy consumption. | Online model update via Bayesian inference to be used for eco-feedback application. | Conference paper: PURDUE 2018. | . . Online Model for Unit-level Heating and Cooling Energy Predicition . . “Online building energy model to evaluate heating and cooling-related behavior changes for eco-feedback in a multifamily residential building.” | Unit-level heating and cooling energy prediction model for actionable eco-feedback design a multi-family residential building. | A single zone gray-box model from thermostat and heat pump power measurement only. | Online parameter learning with state filtering to be used without a whole year dataset in Bayesian framework. | Conference paper: IBPSA 2019. | . . Scalable Building Energy Model using Probabilistic Deep learning . . “A Scalable Building Modeling Approach using Probabilistic Deep learning (tentative).” | A household-level heating and cooling energy prediction model using Deep neural network. | Overcoming the limitation of black-box model through Bayesian update with pre-trained model to be used any types of residential unit. | Generation of multiple simulated data (EnergyPlus) with stochastic noise (Python EnergyPlus plugin) through Eppy package. | Solving probabilistic deep learning model by using Pyro and Pytorch. | In preparation. | . . Liquid desiccant and dew point evaporative cooling based 100% outdoor air system . . Annual energy simulation for liquid-desiccant and dew point evaporative cooling based 100% outdoor system. | Implementation of PI controller, empirical liquid desiccant and dew point evaporative cooling model in a BCVTB co-simulation framework with EnergyPlus to test a new control algorithm (variable air volume and temperature control). | Development of a data-driven empirical model of PEM fuel cell system to be used for LDEOS system as a cogeneration application. | Conference paper: ASHRAE 2016. | Journal papers: ENB 2016 ENB 2015. | . . Indirect evaporative cooler modeling and test chamber design . . Design of environmental chamber to test the performance of indirect evaporative cooler. | Installation of sensors and data collection platform on PLC-based SCADA machine including wet-bulb temperature, airflow chamber. | Design/installation of indirect evaporative cooler with a manufacturing company. | A numerical model of indirect evaporative cooler by using finite difference method to design the prototype. | Journal papers: ENERGY 2016, APEN 2017 | . . Data Center Cooling . . Development various air-side economizers and their annual energy simulation model for data center cooling application. | Development of a simplified server model that can be used for data center cooling energy simulation. | Estimating the impact of air containment in a modular data center through one-to-one scale test bed experiment and CFD simulation. | Development of excel VBA-based tool to estimate rough annual energy consumption of data center cooling for practical use. | Journal papers: APEN 2015, ATE 2015, ENB 2015, ATE 2016. | .",
          "url": "https://ecosang.github.io/blog/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ecosang.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}